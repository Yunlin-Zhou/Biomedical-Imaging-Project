{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import argparse\nimport os\nimport random\nimport logging\nimport numpy as np\nimport time\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport sys\n\nimport torch.distributed as dist\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nimport nibabel as nib","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-10T03:39:48.321061Z","iopub.execute_input":"2023-05-10T03:39:48.321433Z","iopub.status.idle":"2023-05-10T03:39:48.327727Z","shell.execute_reply.started":"2023-05-10T03:39:48.321405Z","shell.execute_reply":"2023-05-10T03:39:48.326739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/raovish6/TABS","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:39:51.004500Z","iopub.execute_input":"2023-05-10T03:39:51.005112Z","iopub.status.idle":"2023-05-10T03:39:51.943093Z","shell.execute_reply.started":"2023-05-10T03:39:51.005079Z","shell.execute_reply":"2023-05-10T03:39:51.941931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys.path.append('/kaggle/working/TABS') ","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:39:53.192200Z","iopub.execute_input":"2023-05-10T03:39:53.192579Z","iopub.status.idle":"2023-05-10T03:39:53.198202Z","shell.execute_reply.started":"2023-05-10T03:39:53.192542Z","shell.execute_reply":"2023-05-10T03:39:53.197299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from Models.TABS_Model import TABS","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:39:54.903523Z","iopub.execute_input":"2023-05-10T03:39:54.903898Z","iopub.status.idle":"2023-05-10T03:39:54.917885Z","shell.execute_reply.started":"2023-05-10T03:39:54.903866Z","shell.execute_reply":"2023-05-10T03:39:54.916943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install gdown\n!gdown https://drive.google.com/uc?id=1Du6N8hr4lcRCjwSYuwLsepzWVXPmdjEr","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:39:56.244975Z","iopub.execute_input":"2023-05-10T03:39:56.245685Z","iopub.status.idle":"2023-05-10T03:40:11.570157Z","shell.execute_reply.started":"2023-05-10T03:39:56.245649Z","shell.execute_reply":"2023-05-10T03:40:11.568920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the raw CT scan\nct_scan_1R = nib.load('/kaggle/input/micro-ct-scans-of-human-cochlea-normalized/normalized_ct_1R.nii')\n\n# Load the corresponding mask\nmask_1R = nib.load('/kaggle/input/micro-ct-scans-of-human-cochlea-normalized/normalized_mask_1R.nii')\n\n# Get the CT scan data\nct_scan_1R_data = ct_scan_1R.get_fdata()\n\n# Get the mask data\nmask_1R_data = mask_1R.get_fdata()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:40:16.333623Z","iopub.execute_input":"2023-05-10T03:40:16.334061Z","iopub.status.idle":"2023-05-10T03:40:36.587887Z","shell.execute_reply.started":"2023-05-10T03:40:16.333996Z","shell.execute_reply":"2023-05-10T03:40:36.586881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CTScanDataset(Dataset):\n    def __init__(self, ct_scan_data, mask_data, cube_size=(80, 80, 80), channels=1):\n        self.ct_scan = ct_scan_data\n        self.mask = mask_data\n        self.cube_size = cube_size\n        self.channels = channels\n\n    def __len__(self):\n        #return len(self.ct_scan)\n        return 120\n\n    def __getitem__(self, idx):\n        # Get random cube of specified size\n        x_start = np.random.randint(0, self.ct_scan.shape[1] - self.cube_size[0])\n        y_start = np.random.randint(0, self.ct_scan.shape[2] - self.cube_size[1])\n        z_start = np.random.randint(0, self.ct_scan.shape[0] - self.cube_size[2])\n        x_end = x_start + self.cube_size[0]\n        y_end = y_start + self.cube_size[1]\n        z_end = z_start + self.cube_size[2]\n        ct_cube = self.ct_scan[z_start:z_end, x_start:x_end, y_start:y_end]\n        mask_cube = self.mask[z_start:z_end, x_start:x_end, y_start:y_end]\n\n        # Add channel dimension\n        if self.channels == 1:\n            ct_cube = torch.unsqueeze(torch.from_numpy(ct_cube).float(), 0)\n            mask_cube = np.repeat(mask_cube[np.newaxis,:,:,:], 5, axis=0)\n            mask_cube = torch.from_numpy(mask_cube).float()\n            \n        else:\n            ct_cube = torch.from_numpy(ct_cube).float()\n            mask_cube = torch.from_numpy(mask_cube).float()\n            \n\n        return ct_cube, mask_cube","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:40:14.391652Z","iopub.execute_input":"2023-05-10T03:40:14.392099Z","iopub.status.idle":"2023-05-10T03:40:14.408112Z","shell.execute_reply.started":"2023-05-10T03:40:14.392058Z","shell.execute_reply":"2023-05-10T03:40:14.407023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_train = CTScanDataset(ct_scan_1R_data, mask_1R_data, cube_size=(80, 80, 80), channels=1)\ndataloader_train = DataLoader(dataset_train, batch_size=3, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:41:08.216786Z","iopub.execute_input":"2023-05-10T03:41:08.217188Z","iopub.status.idle":"2023-05-10T03:41:08.223607Z","shell.execute_reply.started":"2023-05-10T03:41:08.217155Z","shell.execute_reply":"2023-05-10T03:41:08.222418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for ct_scan, mask in dataloader_train:\n    #print(ct_scan.shape, mask.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:39:33.772312Z","iopub.status.idle":"2023-05-10T03:39:33.773300Z","shell.execute_reply.started":"2023-05-10T03:39:33.773071Z","shell.execute_reply":"2023-05-10T03:39:33.773093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the raw CT scan\nct_scan_2R = nib.load('/kaggle/input/micro-ct-scans-of-human-cochlea-normalized/normalized_ct_2R.nii')\n\n# Load the corresponding mask\nmask_2R = nib.load('/kaggle/input/micro-ct-scans-of-human-cochlea-normalized/normalized_mask_2R.nii')\n\n# Get the CT scan data\nct_scan_2R_data = ct_scan_2R.get_fdata()\n\n# Get the mask data\nmask_2R_data = mask_2R.get_fdata()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:41:11.193131Z","iopub.execute_input":"2023-05-10T03:41:11.193809Z","iopub.status.idle":"2023-05-10T03:41:40.081880Z","shell.execute_reply.started":"2023-05-10T03:41:11.193775Z","shell.execute_reply":"2023-05-10T03:41:40.080822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_val = CTScanDataset(ct_scan_2R_data, mask_2R_data, cube_size=(80, 80, 80), channels=1)\ndataloader_val = DataLoader(dataset_val, batch_size=3, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:42:00.745113Z","iopub.execute_input":"2023-05-10T03:42:00.745476Z","iopub.status.idle":"2023-05-10T03:42:00.752471Z","shell.execute_reply.started":"2023-05-10T03:42:00.745448Z","shell.execute_reply":"2023-05-10T03:42:00.749383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for ct_scan, mask in dataloader_val:\n    #print(ct_scan.shape, mask.shape)  # should print (batch_size, channels, height, width, depth)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:39:33.788165Z","iopub.status.idle":"2023-05-10T03:39:33.789231Z","shell.execute_reply.started":"2023-05-10T03:39:33.788952Z","shell.execute_reply":"2023-05-10T03:39:33.788996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Declare variables\ndate = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\nroot = ''\nlr = 0.00001\nweight_decay = 1e-5\namsgrad = True\nseed = 1000\nno_cuda = False\nnum_workers = 4\nbatch_size = 1\nstart_epoch = 0\nend_epoch = 2\ngpu = 0\ngpu_available = '0,1,2'\n\n# Set seed\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\nrandom.seed(seed)\nnp.random.seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:42:03.217750Z","iopub.execute_input":"2023-05-10T03:42:03.218140Z","iopub.status.idle":"2023-05-10T03:42:03.230180Z","shell.execute_reply.started":"2023-05-10T03:42:03.218107Z","shell.execute_reply":"2023-05-10T03:42:03.229171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:42:07.075993Z","iopub.execute_input":"2023-05-10T03:42:07.076847Z","iopub.status.idle":"2023-05-10T03:42:07.140517Z","shell.execute_reply.started":"2023-05-10T03:42:07.076811Z","shell.execute_reply":"2023-05-10T03:42:07.139549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main_worker():\n\n    # Set seed\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    #torch.cuda.set_device(gpu)  # discouraged\n\n    model = TABS(img_dim = 80,\n                 output_ch = 5)\n\n    model.cuda(gpu)\n\n    print('Model Built!')\n\n    # Using adam optimizer (amsgrad variant) with weight decay\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=amsgrad)\n\n    # MSE loss for this task (regression). Using reduction value of sum because we want to specify the number of voxels to divide by (only in the brain map)\n    criterion = nn.MSELoss(reduction='mean')\n    criterion = criterion.cuda(gpu)\n\n    # *************************************************************************\n    # Place train and validation datasets/dataloaders here\n    # *************************************************************************\n    #dataloader_train = DataLoader(dataset_train, batch_size=3, shuffle=True)\n    #dataloader_val = DataLoader(dataset_val, batch_size=3, shuffle=True)\n\n    start_time = time.time()\n\n    # Enable gradient calculation for training\n    torch.set_grad_enabled(True)\n\n    # Declare lists to keep track of training and val losses over the epochs\n    train_global_losses = []\n    val_global_losses = []\n    best_epoch = 0\n\n    print('Start to train!')\n    \n    start_epoch = 0\n    end_epoch = 20\n\n    # Main training/validation loop\n    for epoch in range(start_epoch, end_epoch):\n\n        # Declare lists to keep track of losses and metrics within the epoch\n        train_epoch_losses = []\n        val_epoch_losses = []\n        val_epoch_pcorr = []\n        val_epoch_psnr = []\n        start_epoch = time.time()\n\n        model.train()\n\n        # Loop through train dataloader here.\n        for i, (ct_scan, mask) in enumerate(dataloader_train):\n            adjust_learning_rate(optimizer, epoch, end_epoch, lr)\n\n            # Sample data for the purpose of demonstration\n            #ct_scan = ct_scan.cuda(gpu, non_blocking=True)\n            #mask = mask.cuda(gpu, non_blocking=True)\n            ct_scan, mask = ct_scan.to(device), mask.to(device)\n\n            loss, isolated_images, stacked_brain_map  = get_loss(model, criterion, ct_scan, mask, 'train')\n\n            train_epoch_losses.append(loss.item())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Transition to val mode\n        model.eval()\n\n        with torch.no_grad():\n\n        # Loop through validation dataloader here.\n            for i, (ct_scan, mask) in enumerate(dataloader_val):\n                #ct_scan = ct_scan.cuda(gpu, non_blocking=True)\n                #mask = mask.cuda(gpu, non_blocking=True)\n                ct_scan, mask = ct_scan.to(device), mask.to(device)\n\n                loss, isolated_images, stacked_brain_map  = get_loss(model, criterion, ct_scan, mask, 'val')\n\n                val_epoch_losses.append(loss.item())\n\n                for j in range(0,len(isolated_images)):\n                    #print(isolated_images.shape)\n                    #print(mask.shape)\n                    #print(stacked_brain_map.shape)\n                    cur_pcorr = overall_metrics(isolated_images[j], mask[j], stacked_brain_map[j])\n                    val_epoch_pcorr.append(cur_pcorr)\n\n        end_epoch = time.time()\n\n        # Average train and val loss over every MRI scan in the epoch. Save to global losses which tracks across epochs\n        train_net_loss = sum(train_epoch_losses) / len(train_epoch_losses)\n        val_net_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n        train_global_losses.append(train_net_loss)\n        val_global_losses.append(val_net_loss)\n        pcorr = sum(val_epoch_pcorr) / len(val_epoch_pcorr)\n\n        print('Epoch: {} | Train Loss: {} | Val Loss: {} | Pearson: {}'.format(epoch, train_net_loss, val_net_loss, pcorr))\n\n        checkpoint_dir = root\n        # Save the model if it reaches a new min validation loss\n        if val_global_losses[-1] == min(val_global_losses):\n            print('saving model at the end of epoch ' + str(epoch))\n            best_epoch = epoch\n            file_name = os.path.join(checkpoint_dir, 'TABS_model_epoch_{}_val_loss_{}.pth'.format(epoch, val_global_losses[-1]))\n            # Only save model at higher epochs\n            if epoch > 150:\n                torch.save({\n                    'epoch': epoch,\n                    'state_dict': model.state_dict(),\n                    'optim_dict': optimizer.state_dict(),\n                    },\n                    file_name)\n\n    end_time = time.time()\n    total_time = (end_time - start_time) / 3600\n    print('The total training time is {:.2f} hours'.format(total_time))\n\n    print('----------------------------------The training process finished!-----------------------------------')\n\n    # log_name = os.path.join(args.root, args.protocol, 'loss_log_restransunet.txt')\n    log_name = os.path.join(root, 'loss_log_TABS.txt')\n\n    with open(log_name, \"a\") as log_file:\n        now = time.strftime(\"%c\")\n        log_file.write('================ Loss (%s) ================\\n' % now)\n        log_file.write('best_epoch: ' + str(best_epoch) + '\\n')\n        log_file.write('train_losses: ')\n        log_file.write('%s\\n' % train_global_losses)\n        log_file.write('val_losses: ')\n        log_file.write('%s\\n' % val_global_losses)\n        log_file.write('train_time: ' + str(total_time))\n\n    learning_curve(best_epoch, train_global_losses, val_global_losses)\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Input the best epoch, lists of global (across epochs) train and val losses. Plot learning curve\ndef learning_curve(best_epoch, train_global_losses, val_global_losses):\n    fig, ax1 = plt.subplots(figsize=(12, 8))\n\n    ax1.set_xlabel('Epochs')\n    ax1.set_xticks(np.arange(0, int(len(train_global_losses) + 1), 10))\n\n    ax1.set_ylabel('Loss')\n    ax1.plot(train_global_losses, '-r', label='Training loss', markersize=3)\n    ax1.plot(val_global_losses, '-b', label='Validation loss', markersize=3)\n    ax1.axvline(best_epoch, color='m', lw=4, alpha=0.5, label='Best epoch')\n    ax1.legend(loc='upper left')\n    save_name = 'Learning_Curve_TABS' + '.png'\n    plt.savefig(os.path.join(root, save_name))\n\ndef adjust_learning_rate(optimizer, epoch, max_epoch, init_lr, power=0.9):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = round(init_lr * np.power(1 - (epoch / max_epoch), power), 8)\n\n# Calculate pearson correlation and psnr only between the voxels of the brain map (do by total brain not tissue type during training)\ndef overall_metrics(isolated_image, target, stacked_brain_map):\n    # Flatten the GT, isolated output, and brain mask\n    GT_flattened = torch.flatten(target)\n    iso_flattened = torch.flatten(isolated_image)\n    mask_flattened = torch.flatten(stacked_brain_map)\n\n    # Only save the part of the flattened GT/output that corresponds to nonzero values of the brain mask\n    GT_flattened = GT_flattened[mask_flattened.nonzero(as_tuple=True)]\n    iso_flattened = iso_flattened[mask_flattened.nonzero(as_tuple=True)]\n\n    iso_flattened = iso_flattened.cpu().detach().numpy()\n    GT_flattened = GT_flattened.cpu().detach().numpy()\n\n    pearson = np.corrcoef(iso_flattened, GT_flattened)[0][1]\n\n    return pearson\n\n# Given the model, criterion, input, and GT, this function calculates the loss and returns the isolated output (stripped of background) and brain map\ndef get_loss(model, criterion, ct_scan, mask, mode):\n\n    if mode == 'val':\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        random.seed(seed)\n        np.random.seed(seed)\n\n    # Gen model outputs\n    output = model(ct_scan.float())\n\n    # Construct binary brain map to consider loss only within there\n    input_squeezed = torch.squeeze(ct_scan,dim=1)\n    brain_map = (input_squeezed > -1).float()\n    stacked_brain_map = torch.cat([brain_map.unsqueeze(1)]*5, dim=1)\n\n    # Zero out the background of the segmentation output\n    isolated_images = torch.mul(stacked_brain_map, output)\n\n    # Calculate loss over just the brain map\n    loss = criterion(isolated_images, mask)\n    num_brain_voxels = stacked_brain_map.sum()\n    loss = loss / num_brain_voxels\n\n    return loss, isolated_images, stacked_brain_map\n\nif __name__ == '__main__':\n        os.environ['CUDA_VISIBLE_DEVICES'] = gpu_available\n        assert torch.cuda.is_available(), \"Currently, we only support CUDA version\"\n        torch.backends.cudnn.enabled = True\n        torch.backends.cudnn.benchmark = True\n        main_worker()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:42:10.053144Z","iopub.execute_input":"2023-05-10T03:42:10.053519Z","iopub.status.idle":"2023-05-10T03:49:03.318664Z","shell.execute_reply.started":"2023-05-10T03:42:10.053490Z","shell.execute_reply":"2023-05-10T03:49:03.317706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = torch.load('/kaggle/working/best_model_TABS.pth')","metadata":{"execution":{"iopub.status.busy":"2023-05-10T04:03:27.446641Z","iopub.execute_input":"2023-05-10T04:03:27.447026Z","iopub.status.idle":"2023-05-10T04:03:27.680579Z","shell.execute_reply.started":"2023-05-10T04:03:27.446980Z","shell.execute_reply":"2023-05-10T04:03:27.679585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-05-10T03:39:38.245510Z","iopub.execute_input":"2023-05-10T03:39:38.245888Z","iopub.status.idle":"2023-05-10T03:39:38.252109Z","shell.execute_reply.started":"2023-05-10T03:39:38.245857Z","shell.execute_reply":"2023-05-10T03:39:38.251032Z"},"trusted":true},"execution_count":null,"outputs":[]}]}